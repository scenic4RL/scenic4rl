{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d3aa8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T20:34:22.615495Z",
     "start_time": "2021-05-18T20:34:13.055330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The code is used to train BC imitator, or pretrained GAIL imitator\n",
    "'''\n",
    "\n",
    "import argparse\n",
    "import tempfile\n",
    "import os.path as osp\n",
    "import gym\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4140f22a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T20:36:36.542813Z",
     "start_time": "2021-05-18T20:36:36.530986Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_expert_data_with_rewards(scenario_file, num_interactions=1000, file_name=\"expert_data\", act_ndim=19):\n",
    "    #from gfrl.base.run_my_ppo2 import create_single_scenic_environment\n",
    "    import numpy as np\n",
    "    expert_observations = []\n",
    "    expert_actions = []\n",
    "    expert_rewards = []\n",
    "\n",
    "    gf_env_settings = {\n",
    "        \"stacked\": True,\n",
    "        \"rewards\": 'scoring',\n",
    "        \"representation\": 'extracted',\n",
    "        \"players\": [f\"agent:left_players=1\"],\n",
    "        \"action_set\": \"default\",#\"default\" \"v2\"\n",
    "    }\n",
    "\n",
    "    from scenic.simulators.gfootball.rl_interface import GFScenicEnv\n",
    "    from scenic.simulators.gfootball.utilities.scenic_helper import buildScenario\n",
    "    scenario = buildScenario(scenario_file)\n",
    "    env = GFScenicEnv(initial_scenario=scenario, gf_env_settings=gf_env_settings, use_scenic_behavior_in_step=True, constraints_checking=True)\n",
    "    \n",
    "    obs = env.reset()\n",
    "    tr = 0\n",
    "    for i in tqdm(range(num_interactions)):\n",
    "        expert_observations.append(obs)\n",
    "\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "        tr+=reward\n",
    "        # print(info)\n",
    "        action = info[\"action_taken\"]\n",
    "        expert_actions.append(action)\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            expert_rewards.append(tr)\n",
    "            tr = 0\n",
    "\n",
    "    expert_observations = np.array(expert_observations)\n",
    "    #expert_observations = np.moveaxis(expert_observations, [3], [1])\n",
    "    acts = np.array(expert_actions)\n",
    "\n",
    "    acts_oh = np.zeros((acts.shape[0], act_ndim))\n",
    "    acts_oh[np.arange(acts.shape[0]), acts] = 1\n",
    "\n",
    "    expert_rewards = np.array(expert_rewards)\n",
    "    print(\"Expert observation shape: \", expert_observations.shape)\n",
    "    print(\"Expert actions shape: \", acts_oh.shape)\n",
    "    print(\"Num Episode: \", expert_rewards.shape[0])\n",
    "    print(\"Mean Reward: \", expert_rewards.mean())\n",
    "\n",
    "    #np.savez(expert_file_name, obs=mb_obs, acs=mb_act_oh, num_epi = num_episodes, mean_reward = np.sum(mb_rewards)/num_episodes)\n",
    "    np.savez_compressed(\n",
    "        file_name,\n",
    "        acs=acts_oh,\n",
    "        obs=expert_observations,\n",
    "        num_epi = expert_rewards.shape[0],\n",
    "        mean_reward = expert_rewards.mean(),\n",
    "        rewards = expert_rewards\n",
    "    )\n",
    "    return expert_observations, acts_oh, expert_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4843aac7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T20:37:56.981225Z",
     "start_time": "2021-05-18T20:37:56.962184Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_expert_successful_data(scenario_file, num_interactions=1000, file_name=\"expert_data\", act_ndim=19):\n",
    "    #from gfrl.base.run_my_ppo2 import create_single_scenic_environment\n",
    "    import numpy as np\n",
    "    expert_observations = []\n",
    "    expert_actions = []\n",
    "    expert_rewards = []\n",
    "\n",
    "    gf_env_settings = {\n",
    "        \"stacked\": True,\n",
    "        \"rewards\": 'scoring',\n",
    "        \"representation\": 'extracted',\n",
    "        \"players\": [f\"agent:left_players=1\"],\n",
    "        \"action_set\": \"default\",#\"default\" \"v2\"\n",
    "    }\n",
    "\n",
    "    from scenic.simulators.gfootball.rl_interface import GFScenicEnv\n",
    "    from scenic.simulators.gfootball.utilities.scenic_helper import buildScenario\n",
    "    scenario = buildScenario(scenario_file)\n",
    "    env = GFScenicEnv(initial_scenario=scenario, gf_env_settings=gf_env_settings, use_scenic_behavior_in_step=True, constraints_checking=True)\n",
    "    \n",
    "    \n",
    "    tr = 0\n",
    "    \n",
    "    obs_buf, acts_buf, rew_buf  = [], [], []\n",
    "    policy_rews = []\n",
    "    \n",
    "    with tqdm(total=num_interactions) as pbar:\n",
    "        \n",
    "        while(len(expert_observations)<num_interactions):\n",
    "            \n",
    "            \n",
    "            if len(obs_buf)==0:\n",
    "                obs = env.reset()\n",
    "            \n",
    "            obs_buf.append(obs)\n",
    "            obs, reward, done, info = env.step(env.action_space.sample())\n",
    "            \n",
    "            #rew_buf.append(reward)\n",
    "\n",
    "            tr+=reward\n",
    "            # print(info)\n",
    "            action = info[\"action_taken\"]\n",
    "            acts_buf.append(action)\n",
    "\n",
    "            if done:\n",
    "                \n",
    "                #print(f\"New Epi: {len(obs_buf)} R: {tr}\")\n",
    "                \n",
    "                policy_rews.append(tr)\n",
    "                \n",
    "                if tr>0: \n",
    "                    expert_observations.extend(obs_buf)\n",
    "                    expert_actions.extend(acts_buf)\n",
    "                    expert_rewards.append(tr)\n",
    "                    \n",
    "                    if len(expert_observations)> num_interactions:\n",
    "                        pbar.update(num_interactions)\n",
    "                    else:\n",
    "                        pbar.update(len(obs_buf)) \n",
    "                        \n",
    "                    #print(\"Added new Api. Current Size: \", len(expert_observations))\n",
    "\n",
    "\n",
    "                obs_buf, acts_buf, rew_buf  = [], [], []\n",
    "                obs = env.reset()\n",
    "                tr = 0\n",
    "\n",
    "            \n",
    "    print(\"Collection Done\")        \n",
    "        \n",
    "\n",
    "    expert_observations = np.array(expert_observations)\n",
    "    #expert_observations = np.moveaxis(expert_observations, [3], [1])\n",
    "    acts = np.array(expert_actions)\n",
    "\n",
    "    acts_oh = np.zeros((acts.shape[0], act_ndim))\n",
    "    acts_oh[np.arange(acts.shape[0]), acts] = 1\n",
    "\n",
    "    expert_rewards = np.array(expert_rewards)\n",
    "    \n",
    "    print(\"Expert observation shape: \", expert_observations.shape)\n",
    "    print(\"Expert actions shape: \", acts_oh.shape)\n",
    "    print(\"Num Expert Episode: \", expert_rewards.shape[0])\n",
    "    print(\"Mean Expert Reward: \", expert_rewards.mean())\n",
    "    \n",
    "    print(\"Num Trajectories Collected: \", len(policy_rews))\n",
    "    print(\"Mean Policy Reward: \", np.mean(policy_rews))\n",
    "    \n",
    "\n",
    "    #np.savez(expert_file_name, obs=mb_obs, acs=mb_act_oh, num_epi = num_episodes, mean_reward = np.sum(mb_rewards)/num_episodes)\n",
    "    np.savez_compressed(\n",
    "        file_name,\n",
    "        acs=acts_oh,\n",
    "        obs=expert_observations,\n",
    "        num_epi = expert_rewards.shape[0],\n",
    "        mean_reward = expert_rewards.mean(),\n",
    "        rewards = expert_rewards,\n",
    "        policy_mean_reward = np.mean(policy_rews),\n",
    "        policy_total_trajectories = len(policy_rews)\n",
    "    )\n",
    "    return expert_observations, acts_oh, expert_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d34b15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T20:38:36.341989Z",
     "start_time": "2021-05-18T20:38:21.625628Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ß/codebase/scenic/src/scenic/core/errors.py:160: UserWarning: unable to install sys.excepthook to format Scenic backtraces\n",
      "  warnings.warn('unable to install sys.excepthook to format Scenic backtraces')\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.9.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Environment will ignore actions passed to step() and take action provided by Scenic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "973it [00:13, 74.69it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Done\n",
      "Expert observation shape:  (508, 72, 96, 16)\n",
      "Expert actions shape:  (508, 19)\n",
      "Num Expert Episode:  15\n",
      "Mean Expert Reward:  1.0\n",
      "Num Trajectories Collected:  31\n",
      "Mean Policy Reward:  0.4838709677419355\n"
     ]
    }
   ],
   "source": [
    "scenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\n",
    "\n",
    "num_interactions=500\n",
    "expert_file_name = f\"../_data/pns_success_{num_interactions}.npz\"\n",
    "\n",
    "obs, act, rew = generate_expert_successful_data(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a460b55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T20:39:40.192834Z",
     "start_time": "2021-05-18T20:39:40.158927Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'labels' and 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f84d5c7a2ec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgfrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmybase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGFDset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexpert_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"../_data/pns_success_{num_interactions}.npz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGFDset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpert_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'labels' and 'info'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from gfrl.common.mybase.cloning.dataset import GFDset\n",
    "expert_file_name = f\"../_data/pns_success_{num_interactions}.npz\"\n",
    "dataset = GFDset(expert_file_name)\n",
    "print(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "423ce34b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T20:43:36.102859Z",
     "start_time": "2021-05-18T20:43:35.930634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Obs Shape: (508, 72, 96, 16)\n",
      "Act Shape: (508, 19)\n",
      "num_epi: 15\n",
      "mean_reward: 1.0\n",
      "policy_mean_reward: 0.4838709677419355\n",
      "policy_total_trajectories: 31\n",
      "\n",
      "\n",
      "validation\n",
      "Obs Shape: (0, 72, 96, 16)\n",
      "Act Shape: (0, 19)\n",
      "num_epi: 15\n",
      "mean_reward: 1.0\n",
      "policy_mean_reward: 0.4838709677419355\n",
      "policy_total_trajectories: 31\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = f\"../_data/pns_success_{num_interactions}.npz\"\n",
    "\n",
    "from gfrl.common.mybase.cloning.dataset import get_datasets\n",
    "#path = \"/home/ubuntu/ScenicGFootBall/training/gfrl/_data/sc4rl_fg11v1_rns_success_10000.npz\"\n",
    "tds, vds = get_datasets(path, validation_ratio=0.0)\n",
    "\n",
    "print(\"train\")\n",
    "print(tds.summary())\n",
    "print()\n",
    "\n",
    "print(\"validation\")\n",
    "print(vds.summary())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36815ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#td = np.load(expert_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7810278a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acs', 'obs', 'num_epi', 'mean_reward', 'rewards']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list(td.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f40d8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment will ignore actions passed to step() and take action provided by Scenic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:59<00:00, 83.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert observation shape:  (5000, 72, 96, 16)\n",
      "Expert actions shape:  (5000, 19)\n",
      "Num Episode:  133\n",
      "Mean Reward:  0.6240601503759399\n",
      "133 0.6240601503759399 5000 (5000, 19)\n"
     ]
    }
   ],
   "source": [
    "scenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\n",
    "\n",
    "num_interactions=5000\n",
    "expert_file_name = f\"../_data/pns_{num_interactions}.npz\"\n",
    "\n",
    "obs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)\n",
    "\n",
    "from gfrl.common.mybase.cloning.dataset import GFDset\n",
    "dataset = GFDset(expert_file_name)\n",
    "print(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "732f422e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T20:45:33.548598Z",
     "start_time": "2021-05-18T20:45:33.542436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nscenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\\n\\nnum_interactions=10000\\nexpert_file_name = f\"../_data/pns_{num_interactions}.npz\"\\n\\nobs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "scenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\n",
    "\n",
    "num_interactions=10000\n",
    "expert_file_name = f\"../_data/pns_{num_interactions}.npz\"\n",
    "\n",
    "obs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd8c87c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T20:45:20.498766Z",
     "start_time": "2021-05-18T20:45:20.492213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom gfrl.common.mybase.cloning.dataset import GFDset\\ndataset = GFDset(expert_file_name)\\nprint(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from gfrl.common.mybase.cloning.dataset import GFDset\n",
    "dataset = GFDset(expert_file_name)\n",
    "print(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b27c1107",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T20:45:09.270406Z",
     "start_time": "2021-05-18T20:45:09.263394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nscenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\\n\\nnum_interactions=20000\\nexpert_file_name = f\"../_data/pns_{num_interactions}.npz\"\\n\\nobs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)\\nfrom gfrl.common.mybase.cloning.dataset import GFDset\\ndataset = GFDset(expert_file_name)\\nprint(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "scenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\n",
    "\n",
    "num_interactions=20000\n",
    "expert_file_name = f\"../_data/pns_{num_interactions}.npz\"\n",
    "\n",
    "obs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)\n",
    "from gfrl.common.mybase.cloning.dataset import GFDset\n",
    "dataset = GFDset(expert_file_name)\n",
    "print(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b5351dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T20:45:06.828119Z",
     "start_time": "2021-05-18T20:45:06.812305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nscenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\\n\\nnum_interactions=50000\\nexpert_file_name = f\"../_data/pns_{num_interactions}.npz\"\\n\\nobs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)\\n\\nfrom gfrl.common.mybase.cloning.dataset import GFDset\\ndataset = GFDset(expert_file_name)\\nprint(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "scenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\n",
    "\n",
    "num_interactions=50000\n",
    "expert_file_name = f\"../_data/pns_{num_interactions}.npz\"\n",
    "\n",
    "obs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)\n",
    "\n",
    "from gfrl.common.mybase.cloning.dataset import GFDset\n",
    "dataset = GFDset(expert_file_name)\n",
    "print(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}